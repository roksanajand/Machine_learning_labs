{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f1c78d5",
   "metadata": {},
   "source": [
    "# Volve G&G Dataset\n",
    "\n",
    "The \"Volve\" oil field is an offshore oil and gas field located in the North Sea, approximately 190 kilometers west of Stavanger, Norway. It was discovered in 1993 and production began in 2008. The field is owned and operated by Equinor (formerly known as Statoil), which is one of the largest energy companies in the world. The field has estimated recoverable reserves of around 190 million barrels of oil equivalent, and produces mainly crude oil. Volve is a relatively small field compared to some of the other offshore fields in the North Sea, but it is still an important asset for Equinor and for Norway's overall oil and gas industry.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874f568c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(url=\"https://www.norskpetroleum.no/factpages/3420717.jpg\", width=700)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d6b782",
   "metadata": {},
   "source": [
    "# What kind of well-logs we will use?\n",
    "\n",
    "-  **NPHI** (neutron porosity): It is used to estimate the amount of fluids (usually hydrocarbons) contained in the rock formation by measuring the amount of neutron radiation that is emitted by the rock and reflected back to the sensor. This is important in the oil and gas industry to determine the potential productivity of a reservoir.\n",
    "\n",
    "-  **RHOB** (bulk density): It is used to determine the weight of the rock formation per unit volume. This is important for calculating the overall density of the rock formation and understanding its mechanical properties.\n",
    "\n",
    "-  **GR** (gamma ray): It is used to measure the amount of natural radiation that is emitted by the rock formation. This information can be used to identify certain types of rock formations, such as shale, and to estimate the amount of organic matter present in the formation.\n",
    "\n",
    "-  **RT** (resistivity): It is used to measure the electrical resistance of the rock formation to the flow of electric current. This information can be used to determine the presence and quality of fluids within the rock formation.\n",
    "\n",
    "-  **PEF** (photoelectric factor): It is used to measure the amount of X-ray radiation that is absorbed by the rock formation. This information can be used to identify certain types of rock formations, such as sandstone, and to estimate the amount of organic matter present in the formation.\n",
    "\n",
    "-  **CALI** (caliper): It is used to measure the diameter of the borehole. This information is important for determining the correct size of tools to be used for further measurements and for ensuring the stability of the borehole.\n",
    "\n",
    "-  **DT** (compressional travel time): It is used to measure the time it takes for a compressional (P-wave) sound wave to travel a known distance through the rock formation. This information can be used to determine the rock formation's mechanical properties, such as its porosity and permeability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275b599e",
   "metadata": {},
   "source": [
    "# Step 1 - importing data from Excel file and plotting the logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bb6208",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1368f5",
   "metadata": {},
   "source": [
    "## Task 1 \n",
    "Rread Excel file into a pandas dataframe ans save it to the variable df using function pd.read_excel\n",
    "\n",
    "exaple: variable = pd.read_excel('file_name.xlsx')\n",
    "\n",
    "prinf df - what is the index of your data?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de103c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "\n",
    "put code here\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1839782",
   "metadata": {},
   "source": [
    "## Task 2 \n",
    "\n",
    "Set column DEPTH as the index using the following syntax\n",
    "\n",
    "variable = variable.set_index('column_name')\n",
    "\n",
    "Print the first 10 rows using the syntax\n",
    "\n",
    "variable.head(number of rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f1f9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "\n",
    "put code here\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60c3b0d",
   "metadata": {},
   "source": [
    "## Task 3 import library for ploting or install if needed. Specify the list of colors and plot the logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c60fcc2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set up the plot axes using the number of columns for this put ncols value len(df.columns), set figsize to 15x10\n",
    "fig, axs = plt.subplots(ncols=\"\"\"put sth here\"\"\", figsize=(\"\"\"put sth here\"\"\"), gridspec_kw=dict(wspace=0.9))\n",
    "\n",
    "# Define a list of 7 colors - see website https://matplotlib.org/stable/gallery/color/named_colors.html for ccolor list\n",
    "colors = [color1, color2, .... \n",
    "         , colorN]\n",
    "\n",
    "# Write a loop over the all columns in dataframe, specify linewidth to 0.5\n",
    "for i, col in enumerate(df.columns):\n",
    "    axs[i].plot(df.iloc[:,\"\"\"put here numeric iterator\"\"\"], df.index, color=colors[\"\"\"put here numeric iterator\"\"\"], linewidth=\"\"\"put value here\"\"\")\n",
    "    axs[i].set_xlabel(\"\"\"put here NON-numeric iterator\"\"\")\n",
    "    axs[i].xaxis.label.set_color(colors[\"\"\"put here numeric iterator\"\"\"])\n",
    "    axs[i].set_xlim(df.iloc[:,i].min(), df.iloc[:,\"\"\"put here numeric iterator\"\"\"].max())\n",
    "    axs[i].set_ylabel(\"put y label with the untis in metric system!!!!!\")\n",
    "    axs[i].tick_params(axis='x', colors=colors[i])\n",
    "    axs[i].spines[\"top\"].set_edgecolor(colors[i])\n",
    "    axs[i].title.set_color(colors[i])\n",
    "    axs[i].set_xticks([df.iloc[:,i].min(), df.iloc[:,i].max()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99451f8",
   "metadata": {},
   "source": [
    "## Task 4 Can you notice something strange? What with RT curve? write your comment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b805b82",
   "metadata": {},
   "source": [
    "put\n",
    "\n",
    "\n",
    "comment\n",
    "\n",
    "\n",
    "here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ccf19a5",
   "metadata": {},
   "source": [
    "## Task 5 Check outliers - what is the outlier??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499271fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the plot axes similar as before but use the gig size 20,5\n",
    "fig, axs = plt.subplots(ncols=\"\"\"put sth here\"\"\", figsize=(20,5), \"\"\"put sth here\"\"\"=dict(wspace=0.5))\n",
    "\n",
    "# write a loop function similar as abowe\n",
    "\n",
    "for \"\"\"put sth here\"\"\", \"\"\"put sth here\"\"\" in \"\"\"put sth here\"\"\":\n",
    "    axs[i].boxplot(df[col])\n",
    "    axs[i].set_xlabel(col)\n",
    "    axs[i].set_ylabel(\"Value\")\n",
    "    axs[i].tick_params(axis='x', colors=colors[\"\"\"put sth here\"\"\"])\n",
    "    axs[i].spines[\"top\"].set_edgecolor(colors[\"\"\"put sth here\"\"\"])\n",
    "    axs[i].title.set_color(colors[\"\"\"put sth here\"\"\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f09054",
   "metadata": {},
   "source": [
    "We need to remove weird high values of RT, but what to do with this observations? We can replace them using interpolation from the nearest samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc42e4c",
   "metadata": {},
   "source": [
    "## Task 6 Correct RT curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d3e828",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Replace RT log values greater than 100 with NaN\n",
    "df.loc[df['RT'] > 100, 'RT'] = np.nan\n",
    "\n",
    "# Interpolate NaN values based on nearest values\n",
    "df[\"\"\"put sth here\"\"\"] = df[\"\"\"put sth here\"\"\"].interpolate(method='nearest')\n",
    "\n",
    "# Print the resulting DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b8311a",
   "metadata": {},
   "source": [
    "### See if it helped! We will visualize data again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca149ca0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "put code for the log visualization here again and compare results write your comments\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee73cc9e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "put code for the log visualization here again and compare results write your comments\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027790c0",
   "metadata": {},
   "source": [
    "## Task 7 check the outliers visualization using seaborn library\n",
    "\n",
    "check the seaborn website: https://seaborn.pydata.org/examples/index.html\n",
    "write in the comment 3 visualizations that you can use in your daily tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a19b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "\n",
    "for i, col in enumerate(df.columns):\n",
    "    fig, ax = plt.subplots(figsize=(4, 4))\n",
    "    sns.boxplot(y=df[col], color=colors[i], orient='v')\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27af94c7",
   "metadata": {},
   "source": [
    "write comment\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "HERE!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b04781",
   "metadata": {},
   "source": [
    "## Task 8 - Remove outliers that are 3 standard deviations from the mean in window of 100 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4074c4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set window size to 100\n",
    "window_size =\n",
    "\n",
    "# calculate z-scores for each column using rolling window\n",
    "z_scores = (df - df.rolling(\"\"\"put the variable with window size here\"\"\").mean()) / df.rolling(\"\"\"put the variable with window size here\"\"\").std()\n",
    "\n",
    "# remove rows where any z-score is greater than 3\n",
    "df_noout = df[(np.abs(\"\"\"put the variable with z score\"\"\") < \"\"\"put the number of std dev\"\"\").all(axis=1)]\n",
    "\n",
    "# print cleaned dataframe\n",
    "print(df_noout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874e2777",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Set up the plot axes\n",
    "fig, axs = plt.subplots(ncols=len(df.columns), figsize=(20,5), gridspec_kw=dict(wspace=0.5))\n",
    "\n",
    "\n",
    "# make a for loop for visualization using df dataset \n",
    "\n",
    "for i, col in \n",
    "    \n",
    "# Set up the plot axes\n",
    "fig, axs = plt.subplots(ncols=len(df.columns), figsize=(20,5), gridspec_kw=dict(wspace=0.5))\n",
    "\n",
    "# make a for loop for visualization using df_noout dataset \n",
    "\n",
    "for i, col in "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4bc4b1",
   "metadata": {},
   "source": [
    "# Step 2 - Exploratory Data Analysis\n",
    "EDA (Exploratory Data Analysis) is the process of analyzing and visualizing data in order to extract insights, patterns, and trends. It is typically one of the first steps in data analysis and is used to gain a better understanding of the data and its characteristics. EDA can help identify outliers, missing values, and any other issues with the data that may need to be addressed before further analysis. It can also help in selecting appropriate statistical methods and models for data analysis. EDA involves using a range of techniques such as histograms, scatter plots, box plots, and correlation matrices to explore the data visually and identify relationships between variables. EDA is an important part of data science and plays a crucial role in the data analysis process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4cbbab2",
   "metadata": {},
   "source": [
    "## Histograms\n",
    "\n",
    "Histograms are graphical representations of the distribution of data. They display the frequency distribution of a variable by creating a set of contiguous and non-overlapping intervals (or bins) along the range of the variable and then plotting the count or proportion of observations that fall within each bin. By examining a histogram, one can see the central tendency, variability, and shape of the distribution of the data. Additionally, it can also help identify any outliers or unusual patterns in the data. Overall, histograms are useful for understanding the distribution of a variable and gaining insight into the underlying patterns and characteristics of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1190f319",
   "metadata": {},
   "source": [
    "**Optimal bin size**\n",
    "\n",
    "**Freedman-Diaconis rule** is a method for determining the bin width of a histogram in statistical data analysis. The rule uses the interquartile range (IQR) of the data and the total number of samples to calculate an appropriate bin width. The bin width is important because it determines the smoothness of the histogram and can affect the interpretation of the data. The Freedman-Diaconis rule aims to create a histogram with sufficient detail to reveal the underlying distribution of the data while avoiding oversmoothing or undersmoothing. It is considered a robust method for determining bin width because it is less sensitive to outliers than other methods such as Sturges' rule or Scott's rule. The Freedman-Diaconis rule has been widely adopted in various fields, including economics, environmental science, and medical research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0688ecb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the plot axes as before, use size 20,5\n",
    "fig, axs = plt.subplots(ncols=, figsize=, gridspec_kw=\n",
    "\n",
    "#write a loop \n",
    "for i, col in enumerate(df_noout.columns):\n",
    "                        \n",
    "    # Calculate the bin size using the Freedman-Diaconis rule\n",
    "    # specify the quartiles \n",
    "    q75, q25 = np.percentile(df_noout[col], [75, 25])\n",
    "    # calculate iqr (interquartile range)                \n",
    "    iqr = \n",
    "    \n",
    "    # calculate the width as h equals to double iqr divided by the cube root of the number of observation -> n = len(df[col])                  \n",
    "    h =  \"\"\"put sth here\"\"\" / \"\"\"put sth here\"\"\" ** (1/3))\n",
    "    bins = int((df_noout[col].max() - df_noout[col].min()) / h)\n",
    "\n",
    "    axs[i].hist(df_noout[col], bins=bins, color=colors[i])\n",
    "    axs[i].set_xlabel(col)\n",
    "    axs[i].set_ylabel(\"N\")\n",
    "    axs[i].tick_params(axis='x', colors=colors[i])\n",
    "    axs[i].spines[\"top\"].set_edgecolor(colors[i])\n",
    "    axs[i].title.set_color(colors[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823936cb",
   "metadata": {},
   "source": [
    "## Pairplot\n",
    "A pairplot is a graphical tool used in data analysis and visualization that creates pairwise scatterplots and histograms for a given dataset. It is a useful method for exploring the relationship between multiple variables in a dataset. Each scatterplot in the pairplot shows the relationship between two variables in the dataset, while the histograms show the distribution of each variable individually.\n",
    "\n",
    "By examining the pairplot, we can gain insights into the relationships between variables in the dataset. We can identify variables that are strongly correlated, positively or negatively, as well as variables that are not correlated at all. We can also see the distribution of each variable, including whether they are normally distributed or skewed, and identify any outliers. Pairplots can be useful for identifying potential patterns or trends in the data, as well as for detecting any issues with the data, such as missing or erroneous values. Overall, pairplots are a useful tool for exploratory data analysis and for gaining a better understanding of the relationships between variables in a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e5e82e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cols = ['NPHI', 'RHOB', 'GR', 'RT', 'PEF', 'CALI', 'DT']\n",
    "\n",
    "# Define colors for histograms and scatters\n",
    "hist_color = 'lightgreen'\n",
    "scatter_color = 'lavender'\n",
    "\n",
    "# Set the color palette\n",
    "sns.set_palette(\"pastel\")\n",
    "\n",
    "# Create pairplot with different colors for histograms and scatters put as vars your column list, and use diag_kind as 'kde', specify size s as 20\n",
    "sns.pairplot(df_noout, vars=, diag_kind=,\n",
    "             plot_kws = {'alpha': 0.6, 's': , 'edgecolor': scatter_color},\n",
    "             diag_kws = {'color': hist_color})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298bff69",
   "metadata": {},
   "source": [
    "WRITE YOUR OBSERVATIONS HERE!!\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "WRITE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa64c85",
   "metadata": {},
   "source": [
    "## Correlation matrix \n",
    "\n",
    "Correlation matrix is a table that displays the correlation coefficients between different variables in a dataset. It is commonly used in statistics and data analysis to identify patterns and relationships between variables.\n",
    "\n",
    "The correlation coefficient is a statistical measure that ranges from -1 to 1, indicating the strength and direction of the linear relationship between two variables. A value of 1 indicates a perfect positive correlation, meaning that when one variable increases, the other variable increases proportionally. A value of -1 indicates a perfect negative correlation, meaning that when one variable increases, the other variable decreases proportionally. A value of 0 indicates no correlation, meaning that there is no relationship between the variables.\n",
    "\n",
    "The correlation matrix is a square matrix where the diagonal contains the correlation coefficient between each variable and itself, which is always 1. The upper and lower triangles of the matrix contain the correlation coefficients between each pair of variables, with duplicates reflected across the diagonal. A correlation matrix can be visualized as a heat map, where the color of each cell represents the magnitude of the correlation coefficient. Correlation matrices are commonly used in data analysis, machine learning, and other applications to identify relationships between variables, detect multicollinearity, and perform feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c823c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the features to include in the correlation matrix\n",
    "cols = ['NPHI', 'RHOB', 'GR', 'RT', 'PEF', 'CALI', 'DT']\n",
    "\n",
    "# Calculate the correlation matrix using the following synthax dataframe[columsn].corr()\n",
    "corr_matrix = df_noout[cols].corr()\n",
    "\n",
    "# Set the color palette\n",
    "sns.set_palette(\"pastel\")\n",
    "\n",
    "# Create a heatmap of the correlation matrix, set annot to True\n",
    "sns.heatmap(corr_matrix, annot=, cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d44456",
   "metadata": {},
   "source": [
    "WRITE YOUR OBSERVATIONS HERE!!\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "WRITE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7085f4f9",
   "metadata": {},
   "source": [
    "# Step 3 - data normalization\n",
    "\n",
    "Data normalization, also known as feature scaling, is the process of transforming data into a common scale or range in order to facilitate data analysis and improve the performance of machine learning algorithms. Normalization is important because many machine learning algorithms are sensitive to the scale and distribution of input features, and may perform poorly or inaccurately if the features are not on a similar scale.\n",
    "\n",
    "Normalization involves rescaling the features of a dataset to have a mean of 0 and a standard deviation of 1, or scaling the features to a range between 0 and 1. The normalization method used depends on the specific data and the requirements of the analysis or algorithm being used. Common methods of normalization include Min-Max scaling, Z-score normalization, and Log transformation.\n",
    "\n",
    "Min-Max scaling involves scaling the features to a range between 0 and 1, where the minimum value of the feature is transformed to 0 and the maximum value is transformed to 1. Z-score normalization involves transforming the features so that they have a mean of 0 and a standard deviation of 1, which can be accomplished by subtracting the mean from each value and then dividing by the standard deviation. Log transformation is another normalization technique used for data that is highly skewed or has a wide range of values, and involves applying a logarithmic function to the data to transform it into a more normal distribution.\n",
    "\n",
    "Overall, data normalization is an important preprocessing step in data analysis and machine learning, as it can help to improve the accuracy and performance of models and algorithms, reduce overfitting, and ensure that features are on a similar scale.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b4290d",
   "metadata": {},
   "source": [
    "## 3.1 - Transform Resitivity Log to log scale\n",
    "\n",
    "Resistivity data in well logs is typically measured in ohm-meters, and the values can span several orders of magnitude, making it difficult to visualize and analyze the data directly. To address this issue, resistivity data is often transformed using a logarithmic scale, which compresses the data into a more manageable range.\n",
    "\n",
    "The logarithmic scale is a nonlinear scale that compresses large values into a smaller range, while expanding small values. This allows for a more accurate visualization of the data and makes it easier to identify patterns and trends. In particular, the use of logarithmic scales is useful for resistivity data because the range of resistivity values encountered in well logs can be very large, spanning several orders of magnitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e294012d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_noout['column_name'] = np.log10(df_noout['column_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ece681",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check data\n",
    "df_noout['column_name']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320efbfe",
   "metadata": {},
   "source": [
    "## 3.2 - Transform data with skewed distribution\n",
    "The power transform with Yeo-Johnson method is a data transformation technique used to normalize a dataset that has a skewed distribution. It is a variant of the Box-Cox transformation, which is used to normalize data that has a positive skew.\n",
    "\n",
    "The Yeo-Johnson method is a modified version of the Box-Cox transformation that can be applied to both positively and negatively skewed data, as well as data that includes zero or negative values. It works by applying a power transformation to the data that varies based on the value of a lambda parameter, which is estimated from the data.\n",
    "\n",
    "The Yeo-Johnson method is implemented in the PowerTransformer class of the scikit-learn library in Python. It can be used to transform a pandas DataFrame or numpy array to have a more normal distribution, which can be useful for machine learning models that assume a normal distribution of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238a14e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(url=\"https://i.postimg.cc/sDRksm7T/2.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b127fbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary libraries\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "import pandas as pd\n",
    "\n",
    "#define the columns to be transformed\n",
    "numeric_features = [col1, col2, ..., col3]\n",
    "\n",
    "#define the transformation pipeline using PowerTransformer with Yeo-Johnson method and standardization\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "('scaler', PowerTransformer(method='yeo-johnson', standardize=True))\n",
    "])\n",
    "\n",
    "#define the ColumnTransformer to apply the transformation pipeline to the numeric features\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "('num', numeric_transformer, numeric_features)\n",
    "])\n",
    "\n",
    "#fit and transform the data using the preprocessor\n",
    "transformed_data = preprocessor.fit_transform(df_noout[numeric_features])\n",
    "\n",
    "#convert the transformed data to a DataFrame with column names and add the depth column from the original data\n",
    "transformed_data = pd.DataFrame(transformed_data, columns=numeric_features)\n",
    "transformed_data['DEPTH'] = df_noout.reset_index()['DEPTH']\n",
    "transformed_data = transformed_data.set_index('DEPTH')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c7f537",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf584ec0",
   "metadata": {},
   "source": [
    "## DRAW Pairplot using transformed data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4a7674",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "Write code here\n",
    "\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785282b4",
   "metadata": {},
   "source": [
    "## DRAW boxplots using transformed data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a1a911",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "Write code here\n",
    "\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06ad94a",
   "metadata": {},
   "source": [
    "Write your comments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a83f22",
   "metadata": {},
   "source": [
    "# Step 4 - final outlier removal using ML\n",
    "\n",
    "**The Isolation Forest** algorithm is a type of computer program that helps find \"outliers\" in data. Outliers are data points that are very different from all the other data points. For example, imagine you have a list of test scores from your class, and one student got a score that is much higher or lower than everyone else. That student's score would be an outlier.\n",
    "\n",
    "The Isolation Forest algorithm works by putting each data point in a \"tree\". Each tree has branches that divide the data into smaller and smaller groups. The algorithm keeps dividing the data until each point is in its own group, or until a certain number of groups have been made. This is like playing a game of \"guess who\" where you try to guess a character by asking yes-or-no questions, and keep dividing the characters into smaller groups until you know who the character is.\n",
    "\n",
    "Once the data points are divided into groups, the algorithm looks at how many times each point was in a group with other points. If a point was in a group with other points many times, it is not an outlier. But if a point was in a group by itself many times, it is an outlier.\n",
    "\n",
    "The Isolation Forest algorithm can be useful for finding outliers in data, which can be helpful in many different fields like finance, healthcare, and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ec0c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(url=\"https://miro.medium.com/v2/resize:fit:1100/format:webp/1*D78QLbcwXesymhquuofnOg.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7864ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import IsolationForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8f80ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the numerical columns in your DataFrame\n",
    "numeric_cols = ['NPHI', 'RHOB', 'GR', 'RT', 'PEF', 'CALI', 'DT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629701a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the isolation forest for each column separately\n",
    "isos = {}\n",
    "for col in numeric_cols:\n",
    "    iso = IsolationForest(n_estimators=100, max_samples='auto', contamination=float(0.05), random_state=42)\n",
    "    iso.fit(transformed_data[col].values.reshape(-1, 1))\n",
    "    isos[col] = iso\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8707c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the outliers with the mean of neighboring values for each column in a single loop\n",
    "for col in numeric_cols:\n",
    "    outliers = isos[col].predict(transformed_data[col].values.reshape(-1, 1)) == -1\n",
    "    values = transformed_data[col].values\n",
    "    mean = np.mean(values[~outliers])\n",
    "    for i in range(len(values)):\n",
    "        if outliers[i]:\n",
    "            # Replace outlier with the mean of neighboring values\n",
    "            if i == 0:\n",
    "                values[i] = values[i+1]\n",
    "            elif i == len(values)-1:\n",
    "                values[i] = values[i-1]\n",
    "            else:\n",
    "                values[i] = (values[i-1] + values[i+1])/2\n",
    "    transformed_data[col] = values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d7a736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill any remaining NaN values with interpolated values\n",
    "transformed_data[numeric_cols] = transformed_data[numeric_cols].interpolate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3742af9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Print the resulting DataFrame without outliers\n",
    "print(transformed_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43756723",
   "metadata": {},
   "source": [
    "## DRAW boxplots for transformed_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00821966",
   "metadata": {},
   "source": [
    "# Step 5 - Facies analysis\n",
    "Facies analysis from well logs is an important task in petroleum geology. It involves identifying and classifying the different rock types, or facies, encountered in a well. Traditionally, this has been done by geologists through visual inspection of the well logs. However, with the rise of machine learning techniques, it has become possible to automate this process.\n",
    "\n",
    "One common approach to facies analysis is to use unsupervised clustering algorithms, such as K-means or hierarchical clustering, to group similar sections of the well log together. The goal is to identify natural groupings, or clusters, of log responses that correspond to different facies. Once the clusters have been identified, the geologist can assign each cluster to a specific facies based on their knowledge of the local geology.\n",
    "\n",
    "To perform facies analysis using machine learning clustering, several well logs are typically collected and pre-processed. The logs are usually normalized and scaled to make them comparable, and any missing data is imputed or removed. Features, such as gamma-ray, resistivity, and porosity, are extracted from the logs and used as inputs to the clustering algorithm.\n",
    "\n",
    "Once the features have been extracted, a clustering algorithm is applied to group the sections of the well log that have similar responses. The algorithm assigns each section to a cluster, which can be visualized on a plot. The plot can help identify any clear patterns or trends in the data and help the geologist make sense of the clustering results.\n",
    "\n",
    "The next step is to assign each cluster to a specific facies. This is done by comparing the clustering results to the geological knowledge of the area. The geologist may use other data sources, such as core samples or outcrop data, to help identify the facies associated with each cluster.\n",
    "\n",
    "Once the clusters have been assigned to facies, the results can be used to create a facies log. This log can be used to interpret the geology of the well and to help identify potential hydrocarbon reservoirs or other geological features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7025c90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(url=\"https://interviewquery-cms-images.s3-us-west-1.amazonaws.com/ac5da238-25ab-48ef-839a-407a7b76a167.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2614efc",
   "metadata": {},
   "source": [
    "## K-means \n",
    "is a type of unsupervised learning algorithm used for clustering data points into different groups or clusters based on the similarity of the data points. The algorithm works by iteratively assigning each data point to the nearest cluster center, and then computing the new cluster centers based on the mean of the assigned points. This process is repeated until the cluster centers no longer move significantly.\n",
    "\n",
    "The algorithm requires the user to specify the number of clusters beforehand. The objective of the algorithm is to minimize the sum of squared distances between each data point and its assigned cluster center, which is also known as the Within-Cluster-Sum-of-Squares (WCSS) metric.\n",
    "\n",
    "The k-means algorithm can be divided into three main steps:\n",
    "\n",
    "1. **Initialization:** The algorithm randomly selects k data points to act as initial cluster centers.\n",
    "\n",
    "2. **Assignment:** Each data point is assigned to the nearest cluster center based on the Euclidean distance between the point and the center.\n",
    "\n",
    "3. **Update:** The mean of the data points assigned to each cluster is computed, and this value is used as the new cluster center.\n",
    "\n",
    "These three steps are repeated iteratively until the cluster centers no longer move significantly, or a maximum number of iterations is reached. The final output of the algorithm is the cluster assignments of each data point.\n",
    "\n",
    "One of the main advantages of the k-means algorithm is its simplicity and scalability, which allows it to handle large datasets efficiently. However, the algorithm is sensitive to the initial placement of the cluster centers, and may converge to suboptimal solutions. In addition, the algorithm is not effective when dealing with non-linearly separable data. Nonetheless, k-means is widely used in various applications such as image segmentation, market segmentation, and anomaly detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a144deac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(url=\"https://ds055uzetaobb.cloudfront.net/brioche/uploads/y4KGN92h7r-screen-shot-2016-05-05-at-43007-pm.png?width=2000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9116340a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# define features using cols\n",
    "X = transformed_data[\"\"\"put sth here\"\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323f373f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define model set n_clusters to 10\n",
    "\n",
    "model = KMeans(\"\"\"put sth here\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3d0719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model to data\n",
    "y = model.fit_predict(X); y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb78352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save your data in numeric and string format\n",
    "transformed_data['K10'] = y\n",
    "transformed_data['K10_name'] = \"Facies \"+(transformed_data['K10']+1).astype('str')\n",
    "transformed_data = transformed_data.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f82e8e",
   "metadata": {},
   "source": [
    "## Print transformed_data what do you see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30212bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "K5_graph = transformed_data.copy()\n",
    "K5_graph['x'] = 1\n",
    "sns.set_style(\"white\")\n",
    "sns.set(font_scale = 1.5)\n",
    "plt.rcParams['xtick.major.size'] = 200\n",
    "plt.rcParams['xtick.major.width'] = 40\n",
    "plt.rcParams['xtick.bottom'] = True\n",
    "plt.rcParams['ytick.left'] = True\n",
    "plt.figure(figsize=(3,12))\n",
    "ax = sns.scatterplot(data=K5_graph, x='x', y='DEPTH', hue='K10_name', marker='s', s=50000, edgecolor='None', palette='pastel')\n",
    "plt.tick_params(bottom=False, labelbottom=False)\n",
    "plt.yticks(K5_graph['DEPTH'][::500])\n",
    "plt.legend([],[], frameon=False)\n",
    "plt.ylabel(\"Depth [m]\")\n",
    "plt.xlabel('')\n",
    "plt.title(\"KMeans \\n n = 10\")\n",
    "ax.invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.rcParams.update({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e43a58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(n_clusters, data, cols):\n",
    "    model = KMeans(n_clusters=n_clusters, max_iter=300, random_state=1234)\n",
    "    X = transformed_data[cols]\n",
    "    y = model.fit_predict(X)\n",
    "    SSE = model.inertia_\n",
    "    Silhouette = metrics.silhouette_score(X, y)\n",
    "    CHS = metrics.calinski_harabasz_score(X, y)\n",
    "    DBS = metrics.davies_bouldin_score(X, y)\n",
    "    return {'SSE':SSE, 'Silhouette': Silhouette, 'Calinski_Harabasz': CHS, 'Davies_Bouldin':DBS, 'model':model}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15eef7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cluster_scorer = pd.DataFrame()\n",
    "df_cluster_scorer['n_clusters'] = list(range(2, 21))\n",
    "df_cluster_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f3a72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "\n",
    "df_cluster_scorer['SSE'],df_cluster_scorer['Silhouette'],\\\n",
    "df_cluster_scorer['Calinski_Harabasz'], df_cluster_scorer['Davies_Bouldin'],\\\n",
    "df_cluster_scorer['model'] = zip(*df_cluster_scorer['n_clusters'].map(lambda row: score(row, transformed_data, cols).values()))\n",
    "\n",
    "df_cluster_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0ab734",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({})\n",
    "sns.set()\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "\n",
    "\n",
    "# Plot each graph on its own axis\n",
    "df_cluster_scorer.plot.line(x='n_clusters', y='SSE', title=\"SSE\")\n",
    "df_cluster_scorer.plot.line(x='n_clusters', y='Silhouette', title=\"Silhouette -  Values closer to 1 - optimum n number\")\n",
    "df_cluster_scorer.plot.line(x='n_clusters', y='Calinski_Harabasz', title=\"Calinski-Harabasz - Higher value - optimum n number\")\n",
    "df_cluster_scorer.plot.line(x='n_clusters', y='Davies_Bouldin', title=\"Davies-Bouldin - Values closer to 0 - optimum n number\")\n",
    "\n",
    "#plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c41d36e",
   "metadata": {},
   "source": [
    "## Choose optimum number of clusters and predict!\n",
    "\n",
    "Read about metrics here: https://www.mdpi.com/1996-1073/16/1/493 and describe why did you pick this number"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3892bc7e",
   "metadata": {},
   "source": [
    "Write your OWN code :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed57a80",
   "metadata": {},
   "source": [
    "# Compare your results with the stratigrapy \n",
    "https://www.researchgate.net/publication/332441275_Estimation_of_Pore_Pressure_and_Fracture_Gradient_in_Volve_Field_Norwegian_North_Sea"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
